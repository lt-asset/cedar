check_nan: true
constraints:
  margin:
    default: '1.0'
    descp: Has a default value of 1.
    doc_dtype: float, optional
    dtype:
    - torch.float32
    ndim:
    - '0'
  reduce:
    default: None
    descp: 'Deprecated (see `reduction`). By default, the losses are averaged or summed
      over observations for each minibatch depending on `size_average`. When `reduce`
      is `False`, returns a loss per batch element instead and ignores `size_average`.
      Default: `True`'
    doc_dtype: bool, optional
    dtype:
    - torch.bool
    ndim:
    - '0'
  reduction:
    default: mean
    descp: 'Specifies the reduction to apply to the output: `''none''` | `''mean''`
      | `''sum''`. `''none''`: no reduction will be applied, `''mean''`: the sum of
      the output will be divided by the number of elements in the output, `''sum''`:
      the output will be summed. Note: `size_average` and `reduce` are in the process
      of being deprecated, and in the meantime, specifying either of those two args
      will override `reduction`. Default: `''mean''`'
    doc_dtype: string, optional
    dtype:
    - int
    - string
    enum:
    - mean
    - none
    - sum
    ndim:
    - '0'
    range:
    - '[0,inf)'
  size_average:
    default: None
    descp: 'Deprecated (see `reduction`). By default, the losses are averaged over
      each loss element in the batch. Note that for some losses, there are multiple
      elements per sample. If the field `size_average` is set to `False`, the losses
      are instead summed for each minibatch. Ignored when reduce is `False`. Default:
      `True`'
    doc_dtype: bool, optional
    dtype:
    - torch.bool
    enum:
    - size_average
    ndim:
    - '0'
inputs:
  optional:
  - margin
  - size_average
  - reduce
  - reduction
  required: []
layer_constructor: true
link: https://pytorch.org/docs/stable/nn.html#torch.nn.HingeEmbeddingLoss
package: torch
target: HingeEmbeddingLoss
title: torch.nn.HingeEmbeddingLoss
version: 1.5.0
