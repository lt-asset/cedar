constraints:
  _retain_param_name:
    default: 'True'
    descp: ''
    dtype:
    - torch.bool
    ndim:
    - '0'
  args:
    descp: 'the inputs to the model, e.g., such that `model(*args)` is a valid invocation
      of the model.  Any non-Tensor arguments will be hard-coded into the exported
      model; any Tensor arguments will become inputs of the exported model, in the
      order they occur in args.  If args is a Tensor, this is equivalent to having
      called it with a 1-ary tuple of that Tensor. (Note: passing keyword arguments
      to the model is not currently supported.  Give us a shout if you need it.)'
    doc_dtype: tuple of arguments
    dtype:
    - torch.bool
    ndim:
    - '0'
    structure:
    - tuple
    tensor_t:
    - torch.tensor
  aten:
    default: 'False'
    descp: '[DEPRECATED. use operator_export_type] export the model in aten mode.
      If using aten mode, all the ops original exported by the functions in symbolic_opset<version>.py
      are exported as ATen ops.'
    doc_dtype: bool, default False
    dtype:
    - torch.bool
    ndim:
    - '0'
  custom_opsets:
    default: None
    descp: ''
  do_constant_folding:
    default: 'True'
    descp: If True, the constant-folding optimization is applied to the model during
      export. Constant-folding optimization will replace some of the ops that have
      all constant inputs, with pre-computed constant nodes.
    doc_dtype: bool, default False
    dtype:
    - torch.bool
    ndim:
    - '0'
  dynamic_axes:
    default: None
    descp: ''
    doc_dtype: dict<string, dict<python:int, string>> or dict<string, list(int)>,
      default empty dict
    dtype:
    - int
    - string
    structure:
    - dict
    - list
  enable_onnx_checker:
    default: 'True'
    descp: ''
    dtype:
    - torch.bool
    ndim:
    - '0'
  example_outputs:
    default: None
    descp: Model's example outputs being exported. example_outputs must be provided
      when exporting a ScriptModule or TorchScript Function.
    doc_dtype: tuple of Tensors, default None
    structure:
    - tuple
    tensor_t:
    - torch.tensor
  export_params:
    default: 'True'
    descp: if specified, all parameters will be exported.  Set this to False if you
      want to export an untrained model. In this case, the exported model will first
      take all of its parameters as arguments, the ordering as specified by `model.state_dict().values()`
    doc_dtype: bool, default True
    dtype:
    - torch.bool
    ndim:
    - '0'
  export_raw_ir:
    default: 'False'
    descp: '[DEPRECATED. use operator_export_type] export the internal IR directly
      instead of converting it to ONNX ops.'
    doc_dtype: bool, default False
    dtype:
    - torch.bool
    ndim:
    - '0'
  f:
    descp: a file-like object (has to implement fileno that returns a file descriptor)
      or a string containing a file name.  A binary Protobuf will be written to this
      file.
    ndim:
    - '0'
    - '1'
  input_names:
    default: None
    descp: names to assign to the input nodes of the graph, in order
    doc_dtype: list of strings, default empty list
    dtype:
    - string
    structure:
    - list
  keep_initializers_as_inputs:
    default: None
    descp: ''
  model:
    descp: the model to be exported.
    doc_dtype: torch.nn.Module
  operator_export_type:
    default: None
    descp: 'OperatorExportTypes.ONNX: all ops are exported as regular ONNX ops. OperatorExportTypes.ONNX_ATEN:
      all ops are exported as ATen ops. OperatorExportTypes.ONNX_ATEN_FALLBACK: if
      symbolic is missing, fall back on ATen op. OperatorExportTypes.RAW: export raw
      ir.'
    doc_dtype: enum, default OperatorExportTypes.ONNX
  opset_version:
    default: None
    descp: by default we export the model to the opset version of the onnx submodule.
      Since ONNX's latest opset may evolve before next stable release, by default
      we export to one stable opset version. Right now, supported stable opset version
      is 9. The opset_version must be _onnx_master_opset or in _onnx_stable_opsets
      which are defined in torch/onnx/symbolic_helper.py
    doc_dtype: int, default is 9
    dtype:
    - int
    - torch.bool
    ndim:
    - '0'
  output_names:
    default: None
    descp: names to assign to the output nodes of the graph, in order
    doc_dtype: list of strings, default empty list
    dtype:
    - string
    structure:
    - list
  strip_doc_string:
    default: 'True'
    descp: if True, strips the field "doc_string" from the exported model, which information
      about the stack trace.
    doc_dtype: bool, default True
    dtype:
    - torch.bool
    ndim:
    - '0'
  training:
    default: 'False'
    descp: export the model in training mode.  At the moment, ONNX is oriented towards
      exporting models for inference only, so you will generally not need to set this
      to True.
    doc_dtype: bool, default False
    dtype:
    - torch.bool
    ndim:
    - '0'
  use_external_data_format:
    default: 'False'
    descp: ''
    dtype:
    - torch.bool
    ndim:
    - '0'
  verbose:
    default: 'False'
    descp: if specified, we will print out a debug description of the trace being
      exported.
    doc_dtype: bool, default False
    dtype:
    - torch.bool
    ndim:
    - '0'
inputs:
  optional:
  - export_params
  - verbose
  - training
  - input_names
  - output_names
  - aten
  - export_raw_ir
  - operator_export_type
  - opset_version
  - _retain_param_name
  - do_constant_folding
  - example_outputs
  - strip_doc_string
  - dynamic_axes
  - keep_initializers_as_inputs
  - custom_opsets
  - enable_onnx_checker
  - use_external_data_format
  required:
  - model
  - args
  - f
link: https://pytorch.org/docs/stable/onnx.html#torch.onnx.export
package: torch
target: export
title: torch.onnx.export
version: 1.5.0
