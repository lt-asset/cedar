check_nan: true
constraints:
  activation:
    default: relu
    descp: the activation function of encoder/decoder intermediate layer, relu or
      gelu (default=relu).
    dtype:
    - string
  custom_decoder:
    default: None
    descp: custom decoder (default=None).
  custom_encoder:
    default: None
    descp: custom encoder (default=None).
  d_model:
    default: '512'
    descp: the number of expected features in the encoder/decoder inputs (default=512).
    dtype:
    - int
    ndim:
    - '0'
    range:
    - '[0,inf)'
  dim_feedforward:
    default: '2048'
    descp: the dimension of the feedforward network model (default=2048).
    dtype:
    - int
    ndim:
    - '0'
  dropout:
    default: '0.1'
    descp: the dropout value (default=0.1).
    dtype:
    - torch.float32
    ndim:
    - '0'
  nhead:
    default: '8'
    descp: the number of heads in the multiheadattention models (default=8).
    dtype:
    - int
    ndim:
    - '0'
    range:
    - '[0,inf)'
  num_decoder_layers:
    default: '6'
    descp: the number of sub-decoder-layers in the decoder (default=6).
    dtype:
    - int
    ndim:
    - '0'
    range:
    - '[0,inf)'
  num_encoder_layers:
    default: '6'
    descp: the number of sub-encoder-layers in the encoder (default=6).
    dtype:
    - int
    ndim:
    - '0'
    range:
    - '[0,inf)'
inputs:
  optional:
  - d_model
  - nhead
  - num_encoder_layers
  - num_decoder_layers
  - dim_feedforward
  - dropout
  - activation
  - custom_encoder
  - custom_decoder
  required: []
layer_constructor: true
link: https://pytorch.org/docs/stable/nn.html#torch.nn.Transformer
package: torch
target: Transformer
title: torch.nn.Transformer
version: 1.5.0
