aliases:
- tf.compat.v1.custom_gradient
constraints:
  f:
    default: None
    descp: 'function `f(*x)` that returns a tuple `(y, grad_fn)` where: `x` is a sequence
      of `Tensor` inputs to the function.`y` is a `Tensor` or sequence of `Tensor`
      outputs of applying TensorFlow operations in `f` to `x`.`grad_fn` is a function
      with the signature `g(*grad_ys)` which returns a list of `Tensor`s - the derivatives
      of `Tensor`s in `y` with respect to the `Tensor`s in `x`.  `grad_ys` is a `Tensor`
      or sequence of`Tensor`s the same size as `y` holding the initial value gradients
      for each `Tensor` in `y`. In a pure mathematical sense, a vector-argument vector-valued
      function `f`''s derivatives should be its Jacobian matrix`J`. Here we are expressing
      the Jacobian `J` as a function `grad_fn`which defines how `J` will transform
      a vector `grad_ys` when left-multiplied with it (`grad_ys * J`). This functional
      representation of a matrix is convenient to use for chain-rule calculation (in
      e.g. the back-propagation algorithm).If `f` uses `Variable`s (that are not part
      of the inputs), i.e. through `get_variable`, then `grad_fn` should have signature
      `g(*grad_ys, variables=None)`, where `variables` is a list of the `Variable`s,
      and return a 2-tuple `(grad_xs, grad_vars)`, where`grad_xs` is the same as above,
      and `grad_vars` is a `list<Tensor>`with the derivatives of `Tensor`s in `y`
      with respect to the variables (that is, grad_vars has one Tensor per variable
      in variables). '
    ndim:
    - '0'
    structure:
    - list
inputs:
  optional:
  - f
  required: []
link: https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/custom_gradient
outputs: A function `h(x)` which returns the same value as `f(x)[0]` and whose gradient
  (as calculated by `tf.gradients`) is determined by `f(x)[1]`.
package: tensorflow
target: custom_gradient
title: tf.custom_gradient
version: 2.1.0
