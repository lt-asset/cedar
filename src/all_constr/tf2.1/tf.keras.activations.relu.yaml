aliases:
- tf.compat.v1.keras.activations.relu
constraints:
  alpha:
    default: '0.0'
    descp: A `float` that governs the slope for values lower than the threshold.
    dtype:
    - float
    ndim:
    - '0'
  max_value:
    default: None
    descp: A `float` that sets the saturation threshold (the largest value the function
      will return).
    dtype:
    - float
  threshold:
    default: '0'
    descp: A `float` giving the threshold value of the activation function below which
      values will be damped or set to zero.
    dtype:
    - float
    - int
  x:
    descp: Input `tensor` or `variable`.
    tensor_t:
    - tf.tensor
inputs:
  optional:
  - alpha
  - max_value
  - threshold
  required:
  - x
link: https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/activations/relu
outputs: A `Tensor` representing the input tensor, transformed by the relu activation
  function. Tensor will be of the same shape and dtype of input `x`.
package: tensorflow
target: relu
title: tf.keras.activations.relu
version: 2.1.0
