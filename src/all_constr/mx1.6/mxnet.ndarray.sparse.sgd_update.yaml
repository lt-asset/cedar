constraints:
  '**kwargs':
    descp: ''
  clip_gradient:
    default: _Null
    descp: Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient
      <= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient),
      -clip_gradient).
    doc_dtype: float, optional, default=-1
    dtype:
    - float
    ndim:
    - '0'
  grad:
    default: None
    descp: Gradient
    doc_dtype: NDArray
    structure:
    - ndarray
  lazy_update:
    default: _Null
    descp: If true, lazy updates are applied if gradient's stype is row_sparse.
    doc_dtype: boolean, optional, default=1
    dtype:
    - boolean
    ndim:
    - '0'
  lr:
    default: _Null
    descp: Learning rate
    doc_dtype: float, required
    dtype:
    - float
  name:
    default: None
    descp: ''
  out:
    default: None
    descp: The output NDArray to hold the result.
    doc_dtype: NDArray, optional
    structure:
    - ndarray
  rescale_grad:
    default: _Null
    descp: Rescale gradient to grad = rescale_grad*grad.
    doc_dtype: float, optional, default=1
    dtype:
    - float
    ndim:
    - '0'
  wd:
    default: _Null
    descp: Weight decay augments the objective function with a regularization term
      that penalizes large weights. The penalty scales with the square of the magnitude
      of each weight.
    doc_dtype: float, optional, default=0
    dtype:
    - float
    ndim:
    - '0'
  weight:
    default: None
    descp: Weight
    doc_dtype: NDArray
    structure:
    - ndarray
inputs:
  optional:
  - weight
  - grad
  - lr
  - wd
  - rescale_grad
  - clip_gradient
  - lazy_update
  - out
  - name
  required:
  - '**kwargs'
link: https://mxnet.apache.org/versions/1.6/api/python/docs/api/ndarray/sparse/index.html#mxnet.ndarray.sparse.sgd_update
package: mxnet
target: sgd_update
title: mxnet.ndarray.sparse.sgd_update
version: 1.6.0
